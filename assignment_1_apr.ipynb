{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fc3101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression solves a regression problem where input and output variables have linear relationships While Logistic regression solves a classification problem in which output is classified two or more groups based on input variables.\n",
      "Ex: to classify if student is passed or not based on the numbers scored.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Linear Regression solves a regression problem where input and output variables have linear relationships While Logistic regression solves a classification problem in which output is classified two or more groups based on input variables.\n",
    "Ex: to classify if student is passed or not based on the numbers scored.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3cff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cost function used in Logistic regression is log loss function. It can be optimized using C parametre in sklearn, which is inverse of lamda.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Cost function used in Logistic regression is log loss function. It can be optimized using C parametre in sklearn, which is inverse of lamda.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3671e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regularization helps to prevent overfitting the model by adding a penalty term to log-loss cost function. The common technique are L1 and L2 regualrization.\n",
      "L1 regularization adds the absolute value of coefficients to cost fuction. L2 regularization adds squared value of coefficients to cost function with regularization parametre lamda.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Regularization helps to prevent overfitting the model by adding a penalty term to log-loss cost function. The common technique are L1 and L2 regualrization.\n",
    "L1 regularization adds the absolute value of coefficients to cost fuction. L2 regularization adds squared value of coefficients to cost function with regularization parametre lamda.\n",
    "\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128d98db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC curve is curve showing performance of classification problems. It is graph between true positive rate versus falsi positive rate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "ROC curve is curve showing performance of classification problems. It is graph between true positive rate versus falsi positive rate.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecba4ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There severeal techniques used to select best features in Logistic Regression. Some methods are Recursive Feature Elimination, Wrapper, Filter approach.\n",
      "These techniques remove the unncessary features to predict the outputs and hence result in enhanced accuracy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "There severeal techniques used to select best features in Logistic Regression. Some methods are Recursive Feature Elimination, Wrapper, Filter approach.\n",
    "These techniques remove the unncessary features to predict the outputs and hence result in enhanced accuracy\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4694c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We can handle the imbalanced data with some strategies, like resampling the training datasets, over or undersampling. We can also do Stratified k-fold cross validation technique in which validation datasets have equal contribution from both ouput variables. then model is trained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "We can handle the imbalanced data with some strategies, like resampling the training datasets, over or undersampling. We can also do Stratified k-fold cross validation technique in which validation datasets have equal contribution from both ouput variables. then model is trained.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "751d2aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some common disadvantages of Logistic regression are:\n",
      "-Assumption of linearity between dependent and independent variable\n",
      "-only be used to predict discrete values\n",
      "-tough to obtain complex relationships using logistic regression\n",
      "-It requires average or no multi-collinearity\n",
      "\n",
      "To handle multicoliinearity, we can remove one of the feature showing collinearity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Some common disadvantages of Logistic regression are:\n",
    "-Assumption of linearity between dependent and independent variable\n",
    "-only be used to predict discrete values\n",
    "-tough to obtain complex relationships using logistic regression\n",
    "-It requires average or no multi-collinearity\n",
    "\n",
    "To handle multicoliinearity, we can remove one of the feature showing collinearity.\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
